# -*- coding: utf-8 -*-
"""9321-assn3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ArF-ftO2evUeE_nqHMqaOXAU1NAofqEt

# preprocessing

dev env colab
## Load csv file from Gdrive
"""

# Commented out IPython magic to ensure Python compatibility.
# Mount google drive to read images from colab
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/My Drive/DevEnv/9321

# %cd /content/drive/My Drive/DevEnv/9321
# !wget https://github.com/mysilver/COMP9321-Data-Services/raw/master/20t1/assign3/training.csv
# !wget https://github.com/mysilver/COMP9321-Data-Services/raw/master/20t1/assign3/validation.csv

"""# Start"""

import pandas as pd
import numpy as np
import requests
import json

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

from math import sqrt
import matplotlib.pyplot as plt

# ref: https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas
pd.options.mode.chained_assignment = None  # default='warn'

train_df=pd.read_csv('./training.csv')
validation_df=pd.read_csv('./validation.csv')

train_df

"""## Processing data


|Column|Value  |
|--|--|
|Budget|keep value, not convert|
|Genres|convert to number of genres|
|Homepage|convert to boolean (has a homepage or not)|
|original_language|keep???|
|release_date|convert to year|


[release_year_rita.png](https://i.loli.net/2020/04/16/Ul1LeRQtr6PDM3d.png)

[release_year_me](https://pic.luoxufeiyan.com/uploads/20200416145052.png)
"""

# Only keep certain columns as training set

# keep_col = ['budget', 'homepage', 'original_language', 'release_date']
# conv_num_list = ['cast', 'crew', 'genres', 'keywords', 'production_companies', 'production_countries', 'spoken_languages']
keep_col = ['budget', 'homepage', 'release_date']
conv_num_list = ['genres', 'spoken_languages']
Y_label = 'revenue'

# convert-to num
def conv_to_num(dataset, modify_col):
    for i in range(len(dataset[modify_col])):
        curr_str = dataset[modify_col][i] 
        curr_len = len(json.loads(curr_str))
        dataset[modify_col][i] = curr_len
    return dataset

# preprocessing-homepage
def proc_homepage(dataset):
    modify_col = 'homepage'
    for i in range(len(dataset[modify_col])):
        curr_str = dataset[modify_col][i] 
        # convert to Ture if movie has a homepage
        curr_len = (type(curr_str) is str)
        dataset[modify_col][i] = curr_len
    return dataset

# preprocessing-release_date
def proc_rls_date(dataset):
    modify_col = 'release_date'
    for i in range(len(dataset[modify_col])):
        curr_str = dataset[modify_col][i] 
        # convert to year only
        curr_len = int(curr_str[:4])
        dataset[modify_col][i] = curr_len
    return dataset

# original_language
def proc_orig_lang(dataset):
    all_lang = list(set(dataset['original_language'].values))
    le = LabelEncoder()
    le.fit(all_lang)
    dataset['original_language'] = le.transform(dataset['original_language'])
    return dataset

# budget
def proc_budget(dataset):
    modify_col = 'budget'
    #norm certain cols
    y_df = dataset[modify_col]
    y_norm = (y_df - y_df.mean()) / (y_df.max() - y_df.min())
    dataset[modify_col] = y_norm
    return dataset

## TEst only

# proc_dataset = train_df[keep_col]
# str1 = proc_dataset['spoken_languages'][206]
# len(json.loads(str1))

def proc_data(dataset):
    # add Y label
    selected_col = keep_col + conv_num_list
    selected_col.append(Y_label)
    proc_dataset = dataset[selected_col]
    # proc_dataset = dataset[keep_col]

    # do preprocessing
    #convert to num
    for i in conv_num_list:
        try:
            proc_dataset = conv_to_num(proc_dataset, i)
        except:
            print(i)

    # other processing
    proc_dataset = proc_budget(proc_dataset)
    proc_dataset = proc_homepage(proc_dataset)
    proc_dataset = proc_rls_date(proc_dataset)

    #norm certain cols
    y_df = dataset[Y_label]
    y_norm = (y_df - y_df.mean()) / (y_df.max() - y_df.min())
    proc_dataset[Y_label] = y_norm
    
    return proc_dataset

train_set = proc_data(train_df)
vali_set = proc_data(validation_df)
train_set

"""# Fit models

* https://www.pluralsight.com/guides/non-linear-regression-trees-scikit-learn

* https://github.com/wangye707/PY_DELL/blob/b066ae4bb40273b24d698d66450e4207e3eb6808/git_book-master/chapters/Decision_Tree/decisiontree_regressor.py
"""

keep_col = keep_col + conv_num_list

X_train = train_set[keep_col]
Y_train = train_set[Y_label]

X_valid = vali_set[keep_col]
Y_valid = vali_set[Y_label]



"""### decision tree"""

def dTree(X_train, Y_train):
    model_dtree = DecisionTreeRegressor()
    # dtree = DecisionTreeRegressor()
    model_dtree.fit(X_train, Y_train)

    return model_dtree

"""### random forest"""

def randomForest(X_train, Y_train):
    #RF model
    model_rf = RandomForestRegressor(n_estimators=500, oob_score=True, random_state=100)
    # model_rf = RandomForestRegressor(n_estimators=500)
    model_rf.fit(X_train, Y_train) 

    return model_rf

"""### Adaboost"""

def adaBoost(X_train, Y_train):
    model_ab = AdaBoostRegressor(n_estimators=500)
    model_ab.fit(X_train, Y_train)

    return model_ab

"""### GBRT"""

def gbrt(X_train, Y_train):
    model_gbrt = GradientBoostingRegressor(n_estimators=500)
    model_gbrt.fit(X_train, Y_train)

    return model_gbrt

"""# Evaluate"""

model_dt = dTree(X_train, Y_train)
model_rf = randomForest(X_train, Y_train)
model_ab = adaBoost(X_train, Y_train)
model_gbrt = gbrt(X_train, Y_train)

model = model_dt

for i in range(3):
    if i ==0:
        model = model_gbrt
    if i == 1:
        model = model_rf
    if i ==2:
        model = model_ab

    print("Result for %s is:"%(model))
    # For train set
    Y_train_pred = model.predict(X_train)
    print("Train set result:")
    print('Mean squared Error: ',mean_squared_error(Y_train,Y_train_pred))
    print("Correlation score:%f"%(model.score(X_train,Y_train)))



    # For validation set
    print("Validation set result:")
    Y_valid_pred = model.predict(X_valid)
    print('Mean squared Error: ',mean_squared_error(Y_valid,Y_valid_pred)) 
    print("Correlation score:%f"%(model.score(X_valid,Y_valid)))

    print("\n")



"""# P2

https://blog.csdn.net/u010900574/article/details/52669072
"""

# Only keep certain columns as training set

# keep_col = ['budget', 'homepage', 'original_language', 'release_date']
# conv_num_list = ['cast', 'crew', 'genres', 'keywords', 'production_companies', 'production_countries', 'spoken_languages']
keep_col = ['budget', 'homepage', 'release_date']
conv_num_list = ['genres', 'spoken_languages']
Y_label = 'rating'

def proc_data(dataset):
    # add Y label
    selected_col = keep_col + conv_num_list
    selected_col.append(Y_label)
    proc_dataset = dataset[selected_col]
    # proc_dataset = dataset[keep_col]

    # do preprocessing
    #convert to num
    for i in conv_num_list:
        try:
            proc_dataset = conv_to_num(proc_dataset, i)
        except:
            print(i)

    # other processing
    proc_dataset = proc_budget(proc_dataset)
    proc_dataset = proc_homepage(proc_dataset)
    proc_dataset = proc_rls_date(proc_dataset)

    # #norm certain cols
    # y_df = dataset[Y_label]
    # y_norm = (y_df - y_df.mean()) / (y_df.max() - y_df.min())
    # proc_dataset[Y_label] = y_norm
    
    return proc_dataset

train_set = proc_data(train_df)
vali_set = proc_data(validation_df)
train_set

keep_col = keep_col + conv_num_list

X_train = train_set[keep_col]
Y_train = train_set[Y_label]

X_valid = vali_set[keep_col]
Y_valid = vali_set[Y_label]

from sklearn import tree, svm, naive_bayes,neighbors
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier



clfs = {'svm': svm.SVC(),\
        'decision_tree':tree.DecisionTreeClassifier(),
        'naive_gaussian': naive_bayes.GaussianNB(), \
        # 'naive_mul':naive_bayes.MultinomialNB(),\
        'K_neighbor' : neighbors.KNeighborsClassifier(),\
        'bagging_knn' : BaggingClassifier(neighbors.KNeighborsClassifier(), max_samples=0.5,max_features=0.5), \
        'bagging_tree': BaggingClassifier(tree.DecisionTreeClassifier(), max_samples=0.5,max_features=0.5),
        'random_forest' : RandomForestClassifier(n_estimators=50),\
        'adaboost':AdaBoostClassifier(n_estimators=50),\
        'gradient_boost' : GradientBoostingClassifier(n_estimators=50, learning_rate=1.0,max_depth=1, random_state=10)
        }

def try_different_method(clf):
    clf.fit(X_train,Y_train)
    score = clf.score(X_valid,Y_valid)
    print('the score is :', score)

for clf_key in clfs.keys():
    print('the classifier is :',clf_key)
    clf = clfs[clf_key]
    try_different_method(clf)

